{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867e5032-1a43-4e27-a8d1-8534bdef8c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import string\n",
    "import random\n",
    "import json\n",
    "\n",
    "MIMIC_DATA_DIR = '/path/to/MIMIC-III v1.4'\n",
    "SCAN_REPO_DIR = '/path/to/ScAN'\n",
    "DATASET_DIR = 'Datasets'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cdaa5d",
   "metadata": {},
   "source": [
    "## Read data and annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3111b6b4-a978-40d5-bcd8-6cdaa64a75ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "notes = pd.read_csv(os.path.join(MIMIC_DATA_DIR, 'NOTEEVENTS.csv'))\n",
    "diags = pd.read_csv(os.path.join(MIMIC_DATA_DIR, 'DIAGNOSES_ICD.csv'))\n",
    "diag_dict = pd.read_csv(os.path.join(MIMIC_DATA_DIR, 'D_ICD_DIAGNOSES.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a4ac4d-4a94-41ec-b6d0-aa43bb0fd6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(SCAN_REPO_DIR, 'annotations', 'val_hadm.json'), 'r') as f:\n",
    "    val = json.load(f)\n",
    "with open(os.path.join(SCAN_REPO_DIR, 'annotations', 'train_hadm.json'), 'r') as f:\n",
    "    train = json.load(f)\n",
    "with open(os.path.join(SCAN_REPO_DIR, 'annotations', 'test_hadm.json'), 'r') as f:\n",
    "    test = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd3235c-79ad-4cd5-b6d7-3b95bdd8b63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SA_hadms = [i.split('_')[1] for i in val.keys()] + [i.split('_')[1] for i in train.keys()] + [i.split('_')[1] for i in test.keys()]\n",
    "SA_hadms = [int(item) for item in SA_hadms]\n",
    "len(set(SA_hadms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0a1a4b-051d-4500-aea5-2ec06933d14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "SA_subs = [i.split('_')[0] for i in val.keys()] + [i.split('_')[0] for i in train.keys()] + [i.split('_')[0] for i in test.keys()]\n",
    "SA_subs = [int(item) for item in SA_subs]\n",
    "len(set(SA_subs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8766d7c2-74d0-44f7-b80b-5425676eeea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAwords = ['suicide', 'suicidal', 'self-inflicted', 'overdose', 'poison', 'vehicle', 'drowning']\n",
    "neutralHadms, neutralSubs = [], []\n",
    "allSubs = diags['SUBJECT_ID'].unique()\n",
    "random.shuffle(allSubs)\n",
    "for hd in allSubs:\n",
    "    if len(neutralHadms) == 29:\n",
    "        break\n",
    "    if hd in SA_subs or hd in neutralSubs:\n",
    "        continue\n",
    "    des = diags[diags['SUBJECT_ID'] == hd].merge(diag_dict[['ICD9_CODE','SHORT_TITLE', 'LONG_TITLE']], on='ICD9_CODE')['LONG_TITLE']\n",
    "    des = (' '.join(des)).lower()\n",
    "    SAcheck = False\n",
    "    for w in SAwords:\n",
    "        if w in des:\n",
    "            SAcheck = True\n",
    "            break\n",
    "    if not SAcheck:\n",
    "        hadms = list(set(diags[diags['SUBJECT_ID'] == hd]['HADM_ID']))\n",
    "        chosenHadm = random.choice(hadms)\n",
    "        if len(notes[notes['HADM_ID'] == chosenHadm]) == 0:\n",
    "            continue\n",
    "        neutralHadms.append(str(hd) + '_' + str(chosenHadm))\n",
    "        neutralSubs.append(hd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1e6312-2519-4c50-a0a5-b4b59f4529b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "randHadm = random.choice(neutralHadms)\n",
    "diags[diags['HADM_ID'] == randHadm].merge(diag_dict[['ICD9_CODE','SHORT_TITLE', 'LONG_TITLE']], on='ICD9_CODE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533f4340-2865-4b43-a2e7-11b1803b5cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_neutral = {item:{} for item in neutralHadms[:1000]}\n",
    "val_neutral = {item:{} for item in neutralHadms[1000:]}\n",
    "\n",
    "with open(os.path.join(DATASET_DIR, 'train_neutral_hadm.json'), 'w') as outfile:\n",
    "    json.dump(train_neutral, outfile)\n",
    "with open(os.path.join(DATASET_DIR, 'val_neutral_hadm.json'), 'w') as outfile:\n",
    "    json.dump(val_neutral, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f9cdf7-b2eb-432e-a1f8-805edddafb07",
   "metadata": {},
   "source": [
    "## Assign stay label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbcb959",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_label(folder_path, neutral_folder_path):\n",
    "    labels = {}\n",
    "    \n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        sents = pd.read_csv(file_path)\n",
    "        y, unsure, n = 0, 0, 0\n",
    "        for lb in sents['label']:\n",
    "            if lb == 'SA_positive':\n",
    "                y += 1\n",
    "            elif lb == 'SA_negative':\n",
    "                n += 1\n",
    "            elif lb == 'SA_unsure':\n",
    "                unsure += 1\n",
    "\n",
    "        if y > n and y > unsure:\n",
    "            labels[int(sents.iloc[0]['hadmid'])] = 'SA_positive'\n",
    "        elif n > y and n > unsure:\n",
    "            labels[int(sents.iloc[0]['hadmid'])] = 'SA_negative'\n",
    "        elif unsure > y and unsure > n:\n",
    "            labels[int(sents.iloc[0]['hadmid'])] = 'SA_unsure'\n",
    "        elif y + n + unsure == 0:\n",
    "            labels[int(sents.iloc[0]['hadmid'])] = 'SA_negative'\n",
    "        else:\n",
    "            print(sents.iloc[0]['hadmid'], y, unsure, n)\n",
    "            \n",
    "    for filename in os.listdir(neutral_folder_path):\n",
    "        hadm = int(filename.split('.')[0])\n",
    "        train_label[hadm] = 'SA_negative'\n",
    "\n",
    "    return labels\n",
    "\n",
    "train_label = assign_label(os.path.join(SCAN_REPO_DIR, 'ScAN_segmentation', 'train'),\n",
    "                           os.path.join(DATASET_DIR, 'train_neutral'))\n",
    "val_label = assign_label(os.path.join(SCAN_REPO_DIR, 'ScAN_segmentation', 'val'), \n",
    "                         os.path.join(DATASET_DIR, 'val_neutral'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3969a4-b813-49e0-a65d-be3447a7b61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(DATASET_DIR, 'ScAN_segmentation', 'val_label.json'), 'w') as outfile:\n",
    "    json.dump(train_label, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b772b808-dca7-497f-a3d5-301d8b409e14",
   "metadata": {},
   "source": [
    "## Split validation set\n",
    "\n",
    "Split validation set from the training set and use the original val set as the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30a96a8-d5f2-4d5a-b8c5-cded1b2232bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(DATASET_DIR, 'ScAN_segmentation', 'train_label.json'), 'r') as f:\n",
    "    train_label = json.load(f)\n",
    "    \n",
    "y_stays = [s for s in train_label if train_label[s] == 'SA_positive']\n",
    "unsure_stays = [s for s in train_label if train_label[s] == 'SA_unsure']\n",
    "n_stays = [s for s in train_label if train_label[s] == 'SA_negative']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fab1ff-7757-4fa3-8b53-8c84368f28ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_set = random.sample(y_stays, 36) \\\n",
    "    + random.sample(unsure_stays, 10) \\\n",
    "    + random.sample(n_stays, 130)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6e4238-f7aa-46d9-b09c-1e3bdaf89f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(DATASET_DIR, 'ScAN_segmentation', 'validationHadms.json'), 'r') as f:\n",
    "    val_set = json.load(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
