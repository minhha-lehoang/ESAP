{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb99fd1-9ddd-40a3-88a5-29c866dc7524",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "from statistics import mean \n",
    "import string \n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from sklearn.feature_extraction import _stop_words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import fasttext\n",
    "from nltk import word_tokenize          \n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from preprocess import preprocess_text\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a220c4c-6509-46a9-a77e-e39a42faf43a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd02005-fcb4-4396-b555-36acadc2571c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Single sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb7747f-8bfa-4308-8e07-3131c9bc39a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = 'path/to/Datasets'\n",
    "\n",
    "train_dir = os.path.join(DATA_DIR, 'ScAN_segmentation/train')\n",
    "test_dir = os.path.join(DATA_DIR, 'ScAN_segmentation/val')\n",
    "test_dir_neutral = os.path.join(DATA_DIR, 'ScAN_segmentation/val_neutral')\n",
    "train_dir_neutral = os.path.join(DATA_DIR, 'ScAN_segmentation/train_neutral')\n",
    "with open(os.path.join(DATA_DIR, 'ScAN_segmentation/validationHadms.json')) as f:\n",
    "    validHadms = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa47714-cd81-46cb-943b-1e4ba2e6cf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def labeling_evidence(row):\n",
    "    if pd.isna(row['SA_category']) and pd.isna(row['SI_status']):\n",
    "        label = 0#'no'\n",
    "    elif row['SA_category'] == 'N/A':\n",
    "        label = 'SA_negative'\n",
    "    elif row['SA_category'] == 'unsure':\n",
    "        label = 'SA_unsure'\n",
    "    elif not pd.isna(row['SA_category']):\n",
    "        label = 'SA_positive'\n",
    "    else:\n",
    "        label = 'SI'\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc819f9a-0790-4f37-9fd8-96c58f860d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat([pd.read_csv(os.path.join(train_dir, f), na_values=['', 'nan'], keep_default_na=False) for f in os.listdir(train_dir) if f.split('.')[0] not in validHadms and f.split('.')[1]=='csv'])\n",
    "val_df = pd.concat([pd.read_csv(os.path.join(train_dir, f), na_values=['', 'nan'], keep_default_na=False) for f in os.listdir(train_dir) if f.split('.')[0] in validHadms and f.split('.')[1]=='csv'])\n",
    "test_df = pd.concat([pd.read_csv(os.path.join(test_dir, f), na_values=['', 'nan'], keep_default_na=False) for f in os.listdir(test_dir)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f8d61d-27ce-4912-97a5-7b4fb012139e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['text'] = train_df['text'].apply(preprocess_text)\n",
    "val_df['text'] = val_df['text'].apply(preprocess_text)\n",
    "test_df['text'] = test_df['text'].apply(preprocess_text)\n",
    "\n",
    "train_df['label'] = train_df.apply(labeling_evidence, axis=1)\n",
    "val_df['label'] = val_df.apply(labeling_evidence, axis=1)\n",
    "test_df['label'] = test_df.apply(labeling_evidence, axis=1)\n",
    "\n",
    "train_df['label'] = train_df.apply(lambda x: 1 if x['label'] != 0 else 0, axis=1)\n",
    "val_df['label'] = val_df.apply(lambda x: 1 if x['label'] != 0 else 0, axis=1)\n",
    "test_df['label'] = test_df.apply(lambda x: 1 if x['label'] != 0 else 0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2ca30d-90d6-495f-af6e-1ddd7bbc5594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DROP DUPLICATE FOR labeling_evidence\n",
    "countdf = train_df.groupby(['text', 'label']).size().to_frame(name='count').sort_values(by='count', ascending=False)\n",
    "for (text, label) in countdf.index:\n",
    "    if label == 1 and (text, 1) in countdf.index and (text, 0) in countdf.index:\n",
    "        train_df = train_df[(train_df['text'] != text) | (train_df['label'] == 1)]\n",
    "        \n",
    "train_df = train_df.drop_duplicates(subset='text')\n",
    "val_df = val_df.drop_duplicates(subset='text')\n",
    "test_df = test_df.drop_duplicates(subset='text')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0de6372-fd83-486f-8b40-c22f80951d0e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Multi sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef74c6e2-2960-450e-ad32-f63b5d35707e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = {'hadm': [], 'text': [], 'label': []}\n",
    "val_data = {'hadm': [], 'text': [], 'label': []}\n",
    "test_data = {'hadm': [], 'text': [], 'label': []}\n",
    "sentsInPara = 19\n",
    "\n",
    "def labelPara(labels):\n",
    "    for l in ['SA_positive', 'SA_unsure', 'SA_negative', 'SI']:\n",
    "        if l in labels:\n",
    "            return l\n",
    "    return 0\n",
    "\n",
    "for f in os.listdir(train_dir):\n",
    "    if f.split('.')[1] != 'csv': continue\n",
    "    df = pd.read_csv(os.path.join(train_dir, f), na_values=['', 'nan'], keep_default_na=False) \n",
    "    df['text'] = df['text'].apply(preprocess_text)\n",
    "    df['label'] = df.apply(labeling_evidence, axis=1)\n",
    "    delLocs = []\n",
    "    for i, row in df.iterrows():\n",
    "        if i == 0: continue\n",
    "        if df.loc[i-1]['start_pos'] == row['start_pos'] and df.loc[i-1]['text'] == row['text']:\n",
    "            if row['label'] == 0 or row['label'][:2] == 'SI':\n",
    "                delLocs.append(i)\n",
    "            else:\n",
    "                delLocs.append(i-1)\n",
    "    df = df.loc[[i for i in range(len(df)) if i not in delLocs]]\n",
    "        \n",
    "    if f.split('.')[0] not in validHadms:\n",
    "        countdown = 0\n",
    "        for i in range(sentsInPara, len(df)):\n",
    "            if countdown == 0:\n",
    "                paraLabel = labelPara(list(df.iloc[i-sentsInPara:i+1]['label']))\n",
    "            else: \n",
    "                countdown -= 1\n",
    "                continue\n",
    "                \n",
    "            train_data['hadm'].append((int(df.iloc[i]['hadmid']), df.iloc[i]['start_pos']))\n",
    "            train_data['text'].append(' '.join(df.iloc[i-sentsInPara:i+1]['text']))\n",
    "            train_data['label'].append(paraLabel)\n",
    "            countdown = 14 if paraLabel != 0 else 9\n",
    "    else:\n",
    "        countdown = 0\n",
    "        for i in range(sentsInPara, len(df)):\n",
    "            if countdown == 0:\n",
    "                paraLabel = labelPara(list(df.iloc[i-sentsInPara:i+1]['label']))\n",
    "            else: \n",
    "                countdown -= 1\n",
    "                continue\n",
    "                \n",
    "            val_data['hadm'].append((int(df.iloc[i]['hadmid']), df.iloc[i]['start_pos']))\n",
    "            val_data['text'].append(' '.join(df.iloc[i-sentsInPara:i+1]['text']))\n",
    "            val_data['label'].append(paraLabel)\n",
    "            countdown = 14 if paraLabel != 0 else 9\n",
    "\n",
    "for f in os.listdir(test_dir):\n",
    "    if f.split('.')[1] != 'csv': continue\n",
    "    df = pd.read_csv(os.path.join(test_dir, f), na_values=['', 'nan'], keep_default_na=False) \n",
    "    df['text'] = df['text'].apply(preprocess_text)\n",
    "    df['label'] = df.apply(labeling_evidence, axis=1)\n",
    "    delLocs = []\n",
    "    for i, row in df.iterrows():\n",
    "        if i == 0: continue\n",
    "        if df.loc[i-1]['start_pos'] == row['start_pos'] and df.loc[i-1]['text'] == row['text']:\n",
    "            if row['label'] == 0 or row['label'][:2] == 'SI':\n",
    "                delLocs.append(i)\n",
    "            else:\n",
    "                delLocs.append(i-1)\n",
    "    df = df.loc[[i for i in range(len(df)) if i not in delLocs]]\n",
    "        \n",
    "    countdown = 0\n",
    "    for i in range(sentsInPara, len(df)):\n",
    "        if countdown == 0:\n",
    "            paraLabel = labelPara(list(df.iloc[i-sentsInPara:i+1]['label']))\n",
    "        else: \n",
    "            countdown -= 1\n",
    "            continue\n",
    "\n",
    "        test_data['hadm'].append((int(df.iloc[i]['hadmid']), df.iloc[i]['start_pos']))\n",
    "        test_data['text'].append(' '.join(df.iloc[i-sentsInPara:i+1]['text']))\n",
    "        test_data['label'].append(paraLabel)\n",
    "        countdown = 14 if paraLabel != 0 else 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9937b7-8b12-430f-9eb4-b0003a43a663",
   "metadata": {},
   "outputs": [],
   "source": [
    "## EVIDENCE\n",
    "train_df = pd.DataFrame.from_dict(train_data)\n",
    "val_df = pd.DataFrame.from_dict(val_data)\n",
    "test_df = pd.DataFrame.from_dict(test_data)\n",
    "\n",
    "train_df['label'] = train_df.apply(lambda x: 1 if x['label'] != 0 else 0, axis=1)\n",
    "val_df['label'] = val_df.apply(lambda x: 1 if x['label'] != 0 else 0, axis=1)\n",
    "test_df['label'] = test_df.apply(lambda x: 1 if x['label'] != 0 else 0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbb7fd6-370d-440d-8ec3-9c2270bd4990",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SA TYPES\n",
    "train_df = pd.DataFrame.from_dict(train_data)\n",
    "val_df = pd.DataFrame.from_dict(val_data)\n",
    "test_df = pd.DataFrame.from_dict(test_data)\n",
    "\n",
    "train_df = train_df[train_df['label'] != 0]\n",
    "val_df = val_df[val_df['label'] != 0]\n",
    "test_df = test_df[test_df['label'] != 0]\n",
    "\n",
    "train_df['label'] = train_df.apply(lambda x: 1 if x['label'] == 'SA_negative' else 0, axis=1)\n",
    "val_df['label'] = val_df.apply(lambda x: 1 if x['label'] == 'SA_negative' else 0, axis=1)\n",
    "test_df['label'] = test_df.apply(lambda x: 1 if x['label'] == 'SA_negative' else 0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf042ecf-f19c-4a64-9276-de20eccde6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.drop_duplicates(subset='text')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9111e0a9-9d38-429b-8a2d-4158018c53f9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Train fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce209f5b-dfc8-4462-9267-995843e7a9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([train_df, val_df, test_df])\n",
    "txt = '\\n'.join([l for l in df['text'] if l != '.'])\n",
    "\n",
    "with open(os.path.join(DATA_DIR, \"fasttext\", 'fasttext-train-text.txt'), \"w\") as f:\n",
    "    f.write(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662d20d0-9800-4a36-b9c8-3119f2339541",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model = fasttext.train_unsupervised(os.path.join(DATA_DIR, \"fasttext\", 'fasttext-train-text.txt'), minn=2, dim=100, epoch=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee43cef",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516976c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = nltk.stem.snowball.SnowballStemmer(language='english')\n",
    "ft = fasttext.load_model('tools/fasttext_stemmed.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457ae6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1, w2 = stemmer.stem('sucidal'), stemmer.stem('suicidial')\n",
    "vector1, vector2 = ft.get_word_vector(w1), ft.get_word_vector(w2)\n",
    "np.dot(vector1, vector2) / (np.linalg.norm(vector1) * np.linalg.norm(vector2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76db907-fe71-492d-a6a6-44907b0d9afb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738377d2-e93e-489f-8348-ed8342a53c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4a5d80-84c4-4963-b4ff-95d1cb4cf3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {'seed': 42, 'batch_size': 1, 'epochs': 5, 'log_every': 15, 'lr': 0.003, 'dropout': 0.3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b5a899-6dcf-4053-9def-46d85a766947",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import copy\n",
    "import os\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything(args['seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315088b1-754c-4c2c-9970-f182db5d345b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, dims: list, layers=2, act=nn.LeakyReLU(), dropout_p=0.3, keep_last_layer=False):\n",
    "        super(MLP, self).__init__()\n",
    "        assert len(dims) == layers + 1 \n",
    "        self.layers = layers\n",
    "        self.act = act\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.keep_last = keep_last_layer\n",
    "\n",
    "        self.mlp_layers = nn.ModuleList([])\n",
    "        for i in range(self.layers):\n",
    "            self.mlp_layers.append(nn.Linear(dims[i], dims[i + 1]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.mlp_layers) - 1):\n",
    "            x = self.dropout(self.act(self.mlp_layers[i](x)))\n",
    "        if self.keep_last:\n",
    "            x = self.mlp_layers[-1](x)\n",
    "        else:\n",
    "            x = self.act(self.mlp_layers[-1](x))\n",
    "        return x\n",
    "\n",
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, input_dim=5000, hidden_dim=48, output_dim=0, layers=1, act=nn.LeakyReLU(), dropout=0.3):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        self.bi_lstm = nn.LSTM(input_dim, hidden_dim, proj_size=output_dim, num_layers=layers, batch_first=True, dropout=dropout, bidirectional=True)\n",
    "        self.mlp = MLP([hidden_dim*2, hidden_dim, 1], layers=2, act=act, dropout_p=dropout, keep_last_layer=True)\n",
    "    \n",
    "    def forward(self, x, context=3):\n",
    "        docs = []\n",
    "        for i in range(len(x)):\n",
    "            docs.append(x[max(0, i-context):i+context+1])\n",
    "\n",
    "        x = nn.utils.rnn.pack_sequence(docs, enforce_sorted=False)\n",
    "        x, _ = self.bi_lstm(x)\n",
    "        x = nn.utils.rnn.unpack_sequence(x)\n",
    "        x = torch.stack([xi[i] if i <= context else xi[context] for i, xi in enumerate(x)])\n",
    "        x = self.mlp(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09234a70-5e5d-4058-bfa8-2d86e5a974d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('checkpoints/countVectorizer_sent.pkl', 'rb') as fout:\n",
    "    vect = pickle.load(fout)\n",
    "\n",
    "def train_e2e(train_dataloader, model, optimizer, loss_func):\n",
    "    model.train()\n",
    "    loss, batch_num = 0, 0\n",
    "    print_epo = args['log_every']\n",
    "    refs, preds = [], []\n",
    "\n",
    "    for i, data in enumerate(train_dataloader):\n",
    "        batch_loss, scores, labels = train_e2e_batch(data, model, optimizer, loss_func)\n",
    "        loss += batch_loss\n",
    "        batch_num += 1\n",
    "\n",
    "        refs.extend(labels)\n",
    "        preds.extend([1 if s >= 0.5 else 0 for s in scores])\n",
    "\n",
    "        if i % print_epo == 0:\n",
    "            print(\"Batch {}, Loss: {}\".format(i, loss / batch_num))\n",
    "            \n",
    "    return loss / batch_num, accuracy_score(refs, preds), f1_score(refs, preds), precision_score(refs, preds), recall_score(refs, preds)\n",
    "\n",
    "def train_e2e_batch(hadm, model, optimizer, loss_func):\n",
    "    optimizer.zero_grad()\n",
    "    df = train_df[train_df['hadmid'] == hadm]\n",
    "    texts = df['text']\n",
    "    feature = torch.Tensor(vect.transform(texts).toarray())\n",
    "    labels = torch.Tensor(list(df['label']))\n",
    "        \n",
    "    x = model(feature)\n",
    "    loss = loss_func(x.squeeze(-1), labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.data, torch.sigmoid(x.squeeze(-1)), labels\n",
    "\n",
    "def val_e2e(val_dataloader, model, loss_func, mode='val'):\n",
    "    model.eval()\n",
    "    loss, batch_num = 0, 0\n",
    "    refs, preds = [], []\n",
    "\n",
    "    for i, data in enumerate(val_dataloader):\n",
    "        batch_loss, scores, labels = val_e2e_batch(data, model, loss_func, mode)\n",
    "        loss += batch_loss\n",
    "        batch_num += 1\n",
    "\n",
    "        refs.extend(labels)\n",
    "        preds.extend([1 if s >= 0.5 else 0 for s in scores])\n",
    "        \n",
    "    if mode != 'val':\n",
    "        return loss / batch_num, refs, preds\n",
    "    return loss / batch_num, accuracy_score(refs, preds), f1_score(refs, preds), precision_score(refs, preds), recall_score(refs, preds)\n",
    "    \n",
    "def val_e2e_batch(hadm, model, loss_func, mode):\n",
    "    df = val_df[val_df['hadmid'] == hadm] if mode == 'val' else test_df[test_df['hadmid'] == hadm]\n",
    "    texts = df['text']\n",
    "    feature = torch.Tensor(vect.transform(texts).toarray())\n",
    "    labels = torch.Tensor(list(df['label']))\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        x = model(feature)\n",
    "        loss = loss_func(x.squeeze(-1), labels)\n",
    "    return loss.data, torch.sigmoid(x.squeeze(-1)), labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec90de7-7854-41e9-bd4f-e656ccc75578",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BiLSTM()\n",
    "optimizer = torch.optim.Adam([{'params': model.parameters()}], lr=args['lr'], weight_decay=1e-5)\n",
    "loss_func = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(11))\n",
    "params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab603902-566c-4478-8273-664ed6b2002d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_r2, best_loss = 0, 10000\n",
    "history = {'val_loss': [], 'loss': []}\n",
    "model_state_dicts = {'s': []}\n",
    "trainset, valset = list(train_df['hadmid'].unique()), list(val_df['hadmid'].unique())\n",
    "\n",
    "for i in range(args['epochs']):\n",
    "    print(\"Epoch {}\".format(i))\n",
    "    random.shuffle(trainset)\n",
    "    \n",
    "    for stt in range(0, len(trainset), 150):\n",
    "        trainss = trainset[stt:stt+150]\n",
    "        loss, acc, F, P, R = train_e2e(trainss, model, optimizer, loss_func)\n",
    "        history['loss'].append(loss.cpu())\n",
    "        print(\"At Epoch {}, Train Loss: {}, F: {}, P: {}, R: {}\".format(i, loss, F, P, R))\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        loss, acc, F, P, R = val_e2e(valset, model, loss_func)\n",
    "        torch.cuda.empty_cache()\n",
    "        history['val_loss'].append(loss.cpu())\n",
    "        print(\"At Epoch {}, Val Loss: {}, F: {}, P: {}, R: {}\".format(len(model_state_dicts['s']), loss, F, P, R))\n",
    "\n",
    "        model_state_dicts['s'].append(copy.deepcopy(model.state_dict()))\n",
    "        if loss < best_loss:\n",
    "            # model_save_path = os.path.join(model_save_root_path, \"e_{}_{}.mdl\".format(i, rouge2_score))\n",
    "    #         torch.save(model_state_dicts[-1], model_save_path)\n",
    "            best_loss = loss\n",
    "            print(\"Epoch {} stt {} Has best R2 Score of {}\".format(i, len(model_state_dicts['s'])-1, best_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8603972-0cdc-49d9-a9ce-1161b3ac4d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history['loss'][:])\n",
    "plt.plot(history['val_loss'][:])\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training', 'Valid'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178f69f1-36b9-4b09-8c38-c4e7da2baa30",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('checkpoints/lstm_5000_sent.mdl'), strict=False)\n",
    "loss, refs, preds = val_e2e(list(test_df['hadmid'].unique()), model, loss_func, mode='test')\n",
    "print(classification_report(refs, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ed0099-0ba0-476f-8563-45f14d9076a5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## LSTM + Fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49725e97-80c3-4cbe-920b-9ef6b955d6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {'seed': 42, 'batch_size': 256, 'epochs': 50, 'log_every': 7, 'lr': 0.005, 'dropout': 0.3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474c703c-c37b-40c4-81a9-75612497a43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, input_dim=300, hidden_dim=64, output_dim=0, layers=1, act=nn.LeakyReLU(), dropout=0.3):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bi_lstm = nn.LSTM(input_dim, hidden_dim, proj_size=output_dim, num_layers=layers, batch_first=True, dropout=dropout, bidirectional=True)\n",
    "        self.mlp = MLP([hidden_dim*2, hidden_dim, 1], layers=2, act=act, dropout_p=dropout, keep_last_layer=True)\n",
    "    \n",
    "    def forward(self, texts):\n",
    "        docs = []\n",
    "        for t in texts:\n",
    "            words = [stemmer.stem(w) for w in word_tokenize(t)]\n",
    "            vector = torch.stack([torch.from_numpy(ft.get_word_vector(w)) for w in words])\n",
    "            docs.append(vector)\n",
    "\n",
    "        x = nn.utils.rnn.pack_sequence(docs, enforce_sorted=False)\n",
    "        _, (hn, cn) = self.bi_lstm(x)\n",
    "        x = hn.permute(1, 2, 0).reshape(len(texts), self.hidden_dim*2)\n",
    "        x_copy = x.clone()\n",
    "        x = self.mlp(x)\n",
    "        return x, x_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4091671e-90e6-4e52-a50d-c66ae1c83505",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft = fasttext.load_model('tools/fasttext_stemmed_300.bin')\n",
    "stemmer = nltk.stem.snowball.SnowballStemmer(language='english')\n",
    "\n",
    "def train_e2e(train_dataloader, model, optimizer, loss_func):\n",
    "    model.train()\n",
    "    loss, batch_num = 0, 0\n",
    "    print_epo = args['log_every']\n",
    "    refs, preds = [], []\n",
    "\n",
    "    for i in range(0, len(train_dataloader), args['batch_size']):\n",
    "        data = train_dataloader[i:i+args['batch_size']]\n",
    "        batch_loss, scores, labels = train_e2e_batch(data, model, optimizer, loss_func)\n",
    "        loss += batch_loss\n",
    "        batch_num += 1\n",
    "\n",
    "        refs.extend(labels)\n",
    "        preds.extend([1 if s >= 0.5 else 0 for s in scores])\n",
    "\n",
    "        if batch_num % print_epo == 0:\n",
    "            print(\"Batch {}, Loss: {}\".format(batch_num, loss / batch_num))\n",
    "            \n",
    "    return loss / batch_num, f1_score(refs, preds), precision_score(refs, preds), recall_score(refs, preds)\n",
    "\n",
    "def train_e2e_batch(data, model, optimizer, loss_func):\n",
    "    optimizer.zero_grad()\n",
    "    texts = list(data['text'])\n",
    "    labels = torch.Tensor(list(data['label']))\n",
    "        \n",
    "    x = model(texts)\n",
    "    loss = loss_func(x.squeeze(-1), labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.data, torch.sigmoid(x.squeeze(-1)), labels\n",
    "\n",
    "def val_e2e(val_dataloader, model, loss_func, mode='val'):\n",
    "    model.eval()\n",
    "    loss, batch_num = 0, 0\n",
    "    refs, preds = [], []\n",
    "\n",
    "    for i in range(0, len(val_dataloader), args['batch_size']):\n",
    "        data = val_dataloader[i:i+args['batch_size']]\n",
    "        batch_loss, scores, labels = val_e2e_batch(data, model, loss_func, mode)\n",
    "        loss += batch_loss\n",
    "        batch_num += 1\n",
    "\n",
    "        refs.extend(labels)\n",
    "        preds.extend([1 if s >= 0.5 else 0 for s in scores])\n",
    "        \n",
    "    if mode != 'val':\n",
    "        return loss / batch_num, refs, preds\n",
    "    return loss / batch_num, f1_score(refs, preds), precision_score(refs, preds), recall_score(refs, preds)\n",
    "    \n",
    "text2vec = {}\n",
    "def val_e2e_batch(data, model,  loss_func, mode):\n",
    "    texts = list(data['text'])\n",
    "    labels = torch.Tensor(list(data['label']))\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        x, x_copy = model(texts)\n",
    "        loss = loss_func(x.squeeze(-1), labels)\n",
    "        \n",
    "    for tex, vec in zip(texts, x_copy):\n",
    "        if tex not in text2vec:\n",
    "            text2vec[tex] = vec.numpy()\n",
    "    return loss.data, torch.sigmoid(x.squeeze(-1)), labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fec0a6-9a22-49f7-937a-073299fd24fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BiLSTM()\n",
    "optimizer = torch.optim.Adam([{'params': model.parameters()}], lr=args['lr'], weight_decay=1e-5)\n",
    "loss_func = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(11))\n",
    "params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1be7f0-4401-4e0f-826f-66457cf34dd4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_r2, best_loss = 0, 10000\n",
    "history = {'val_loss': [], 'loss': []}\n",
    "model_state_dicts = {'s': []}\n",
    "trainset, valset = train_df.copy(), val_df.copy()\n",
    "\n",
    "for i in range(args['epochs']):\n",
    "    print(\"Epoch {}\".format(i))\n",
    "    trainset.sample(frac=1)\n",
    "    \n",
    "    for stt in range(0, len(trainset), 10000):\n",
    "        trainss = trainset.iloc[stt:stt+10000]\n",
    "        loss, F, P, R = train_e2e(trainss, model, optimizer, loss_func)\n",
    "        history['loss'].append(loss.cpu())\n",
    "        print(\"At Epoch {}, Train Loss: {}, F: {}, P: {}, R: {}\".format(i, loss, F, P, R))\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        loss, F, P, R = val_e2e(valset, model, loss_func)\n",
    "        torch.cuda.empty_cache()\n",
    "        history['val_loss'].append(loss.cpu())\n",
    "        print(\"At Epoch {}, Val Loss: {}, F: {}, P: {}, R: {}\".format(len(model_state_dicts['s']), loss, F, P, R))\n",
    "\n",
    "        model_state_dicts['s'].append(copy.deepcopy(model.state_dict()))\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            print(\"Epoch {} stt {} Has best R2 Score of {}\".format(i, len(model_state_dicts['s'])-1, best_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449338f5-5923-48f0-bf6f-998247a9e674",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('checkpoints/evidence_lstm_fasttext.mdl'), strict=False)\n",
    "loss, refs, preds = val_e2e(train_df, model, loss_func, mode='test')\n",
    "print(classification_report(refs, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8b69ed-14ae-4054-a8e0-f44b78283846",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('checkpoints/evidence_text2vec.pkl', 'wb') as fout:\n",
    "    pickle.dump(text2vec, fout)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
