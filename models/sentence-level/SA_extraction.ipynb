{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb99fd1-9ddd-40a3-88a5-29c866dc7524",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "from statistics import mean \n",
    "import string \n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from sklearn.feature_extraction import _stop_words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import fasttext\n",
    "from nltk import word_tokenize          \n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "from preprocess import preprocess_text\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a220c4c-6509-46a9-a77e-e39a42faf43a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd02005-fcb4-4396-b555-36acadc2571c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Single sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb7747f-8bfa-4308-8e07-3131c9bc39a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = 'path/to/Datasets'\n",
    "\n",
    "train_dir = os.path.join(DATA_DIR, 'ScAN_segmentation/train')\n",
    "test_dir = os.path.join(DATA_DIR, 'ScAN_segmentation/val')\n",
    "test_dir_neutral = os.path.join(DATA_DIR, 'ScAN_segmentation/val_neutral')\n",
    "train_dir_neutral = os.path.join(DATA_DIR, 'ScAN_segmentation/train_neutral')\n",
    "with open(os.path.join(DATA_DIR, 'ScAN_segmentation/validationHadms.json')) as f:\n",
    "    validHadms = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa47714-cd81-46cb-943b-1e4ba2e6cf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def labeling_evidence(row):\n",
    "    if pd.isna(row['SA_category']) and pd.isna(row['SI_status']):\n",
    "        label = 0#'no'\n",
    "    elif row['SA_category'] == 'N/A':\n",
    "        label = 'SA_negative'\n",
    "    elif row['SA_category'] == 'unsure':\n",
    "        label = 'SA_unsure'\n",
    "    elif not pd.isna(row['SA_category']):\n",
    "        label = 'SA_positive'\n",
    "    else:\n",
    "        label = 'SI'\n",
    "    return label\n",
    "\n",
    "def labeling_evidence_noEvidence(row):\n",
    "    if pd.isna(row['SA_category']) and pd.isna(row['SI_status']):\n",
    "        label = 'SI'\n",
    "    elif row['SA_category'] == 'N/A':\n",
    "        label = 'SA_negative'\n",
    "    elif row['SA_category'] == 'unsure':\n",
    "        label = 'SA_unsure'\n",
    "    elif not pd.isna(row['SA_category']):\n",
    "        label = 'SA_positive'\n",
    "    else:\n",
    "        label = 'SI'\n",
    "    return label\n",
    "\n",
    "def labeling_evidence_SI(row):\n",
    "    if pd.isna(row['SA_category']) and pd.isna(row['SI_status']):\n",
    "        label = 0#'no'\n",
    "    elif row['SI_status'] == 'present':\n",
    "        label = 'SI_positive'\n",
    "    elif not pd.isna(row['SI_status']):\n",
    "        label = 'SI_negative'\n",
    "    else:\n",
    "        label = 'SA'\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc819f9a-0790-4f37-9fd8-96c58f860d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat([pd.read_csv(os.path.join(train_dir, f), na_values=['', 'nan'], keep_default_na=False) for f in os.listdir(train_dir) if f.split('.')[0] not in validHadms and f.split('.')[1]=='csv'])\n",
    "val_df = pd.concat([pd.read_csv(os.path.join(train_dir, f), na_values=['', 'nan'], keep_default_na=False) for f in os.listdir(train_dir) if f.split('.')[0] in validHadms and f.split('.')[1]=='csv'])\n",
    "test_df = pd.concat([pd.read_csv(os.path.join(test_dir, f), na_values=['', 'nan'], keep_default_na=False) for f in os.listdir(test_dir)])\n",
    "test_df = pd.concat([test_df]+[pd.read_csv(os.path.join(test_dir_neutral, f), na_values=['', 'nan'], keep_default_na=False) for f in os.listdir(test_dir_neutral)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92dac22-fccd-41b8-8f25-e52b11c31992",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removePresentSAannot(df):\n",
    "    del_text = df[df['label'].isin(['SA_positive', 'SA_negative', 'SA_unsure'])]['text'].drop_duplicates()\n",
    "    data = df[~(df['label'].isin(['SA_positive', 'SA_negative', 'SA_unsure'])) & ~(df['text'].isin(del_text))]\n",
    "    return data\n",
    "\n",
    "test_df = removePresentSAannot(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f8d61d-27ce-4912-97a5-7b4fb012139e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['text'] = train_df['text'].apply(preprocess_text)\n",
    "val_df['text'] = val_df['text'].apply(preprocess_text)\n",
    "test_df['text'] = test_df['text'].apply(preprocess_text)\n",
    "\n",
    "train_df['label'] = train_df.apply(labeling_evidence, axis=1)\n",
    "val_df['label'] = val_df.apply(labeling_evidence, axis=1)\n",
    "test_df['label'] = test_df.apply(labeling_evidence, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c182cfd-6a29-4e00-95e9-2a8bf7af262d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeDuplicateSASI(df, removeType='SI'):\n",
    "    delLocs = []\n",
    "    for i in range(len(df)):\n",
    "        if i == 0: continue\n",
    "        row = df.iloc[i]\n",
    "        if df.iloc[i-1]['start_pos'] == row['start_pos'] and df.iloc[i-1]['end_pos'] == row['end_pos'] and df.iloc[i-1]['text'] == row['text']:\n",
    "            if row['label'] == 0 or row['label'][:2] == removeType:\n",
    "                delLocs.append(i)\n",
    "            else:\n",
    "                delLocs.append(i-1)        \n",
    "    df = df.iloc[[i for i in range(len(df)) if i not in delLocs]]\n",
    "    return df\n",
    "\n",
    "train_df = removeDuplicateSASI(train_df)\n",
    "val_df = removeDuplicateSASI(val_df)\n",
    "test_df = removeDuplicateSASI(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2ca30d-90d6-495f-af6e-1ddd7bbc5594",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.drop_duplicates(subset='text')\n",
    "val_df = val_df.drop_duplicates(subset='text')\n",
    "test_df = test_df.drop_duplicates(subset='text')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0de6372-fd83-486f-8b40-c22f80951d0e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Multi sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef74c6e2-2960-450e-ad32-f63b5d35707e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = {'hadm': [], 'text': [], 'label': []}\n",
    "val_data = {'hadm': [], 'text': [], 'label': []}\n",
    "test_data = {'hadm': [], 'text': [], 'label': []}\n",
    "sentsInPara = 2\n",
    "\n",
    "def checkNoEvidenceInPara(labels):\n",
    "    for l in labels:\n",
    "        if l != 0:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "for f in os.listdir(train_dir):\n",
    "    if f.split('.')[1] != 'csv': continue\n",
    "    df = pd.read_csv(os.path.join(train_dir, f), na_values=['', 'nan'], keep_default_na=False) \n",
    "    df['text'] = df['text'].apply(preprocess_text)\n",
    "    df['label'] = df.apply(labeling_evidence, axis=1)\n",
    "    delLocs = []\n",
    "    for i, row in df.iterrows():\n",
    "        if i == 0: continue\n",
    "        if df.loc[i-1]['start_pos'] == row['start_pos'] and df.loc[i-1]['text'] == row['text']:\n",
    "            if row['label'] == 0 or row['label'][:2] == 'SI':\n",
    "                delLocs.append(i)\n",
    "            else:\n",
    "                delLocs.append(i-1)\n",
    "    df = df.loc[[i for i in range(len(df)) if i not in delLocs]]\n",
    "        \n",
    "    if f.split('.')[0] not in validHadms:\n",
    "        countdown = 0\n",
    "        for i in range(len(df)):\n",
    "            if i < sentsInPara:\n",
    "                if df.iloc[i]['label'] != 0:\n",
    "                    train_data['hadm'].append((int(df.iloc[i]['hadmid']), df.iloc[i]['start_pos']))\n",
    "                    train_data['text'].append(' '.join(df.iloc[:i+1]['text']))\n",
    "                    train_data['label'].append(df.iloc[i]['label'])\n",
    "                continue\n",
    "            \n",
    "            if df.iloc[i]['label'] != 0:\n",
    "                train_data['hadm'].append((int(df.iloc[i]['hadmid']), df.iloc[i]['start_pos']))\n",
    "                train_data['text'].append(' '.join(df.iloc[i-sentsInPara:i+1]['text']))\n",
    "                train_data['label'].append(df.iloc[i]['label'])\n",
    "                countdown = 0\n",
    "            elif countdown == 0 and checkNoEvidenceInPara(df.iloc[i-sentsInPara:i+1]['label']):\n",
    "                train_data['hadm'].append((int(df.iloc[i]['hadmid']), df.iloc[i]['start_pos']))\n",
    "                train_data['text'].append(' '.join(df.iloc[i-sentsInPara:i+1]['text']))\n",
    "                train_data['label'].append(0)\n",
    "                countdown = sentsInPara\n",
    "            elif countdown > 0:\n",
    "                countdown -= 1\n",
    "    else:\n",
    "        countdown = 0\n",
    "        for i in range(len(df)):\n",
    "            if i < sentsInPara:\n",
    "                if df.iloc[i]['label'] != 0:\n",
    "                    val_data['hadm'].append((int(df.iloc[i]['hadmid']), df.iloc[i]['start_pos']))\n",
    "                    val_data['text'].append(' '.join(df.iloc[:i+1]['text']))\n",
    "                    val_data['label'].append(df.iloc[i]['label'])\n",
    "                continue\n",
    "                \n",
    "            if df.iloc[i]['label'] != 0:\n",
    "                val_data['hadm'].append((int(df.iloc[i]['hadmid']), df.iloc[i]['start_pos']))\n",
    "                val_data['text'].append(' '.join(df.iloc[i-sentsInPara:i+1]['text']))\n",
    "                val_data['label'].append(df.iloc[i]['label'])\n",
    "                countdown = 0\n",
    "            elif countdown == 0 and checkNoEvidenceInPara(df.iloc[i-sentsInPara:i+1]['label']):\n",
    "                val_data['hadm'].append((int(df.iloc[i]['hadmid']), df.iloc[i]['start_pos']))\n",
    "                val_data['text'].append(' '.join(df.iloc[i-sentsInPara:i+1]['text']))\n",
    "                val_data['label'].append(0)\n",
    "                countdown = sentsInPara\n",
    "            elif countdown > 0:\n",
    "                countdown -= 1\n",
    "\n",
    "for f in os.listdir(test_dir):\n",
    "    if f.split('.')[1] != 'csv': continue\n",
    "    df = pd.read_csv(os.path.join(test_dir, f), na_values=['', 'nan'], keep_default_na=False) \n",
    "    df['text'] = df['text'].apply(preprocess_text)\n",
    "    df['label'] = df.apply(labeling_evidence, axis=1)\n",
    "    delLocs = []\n",
    "    for i, row in df.iterrows():\n",
    "        if i == 0: continue\n",
    "        if df.loc[i-1]['start_pos'] == row['start_pos'] and df.loc[i-1]['text'] == row['text']:\n",
    "            if row['label'] == 0 or row['label'][:2] == 'SI':\n",
    "                delLocs.append(i)\n",
    "            else:\n",
    "                delLocs.append(i-1)\n",
    "    df = df.loc[[i for i in range(len(df)) if i not in delLocs]]\n",
    "        \n",
    "    countdown = 0\n",
    "    for i in range(len(df)):\n",
    "        if i < sentsInPara:\n",
    "            if df.iloc[i]['label'] != 0:\n",
    "                test_data['hadm'].append((int(df.iloc[i]['hadmid']), df.iloc[i]['start_pos']))\n",
    "                test_data['text'].append(' '.join(df.iloc[:i+1]['text']))\n",
    "                test_data['label'].append(df.iloc[i]['label'])\n",
    "            continue\n",
    "\n",
    "        if df.iloc[i]['label'] != 0:\n",
    "            test_data['hadm'].append((int(df.iloc[i]['hadmid']), df.iloc[i]['start_pos']))\n",
    "            test_data['text'].append(' '.join(df.iloc[i-sentsInPara:i+1]['text']))\n",
    "            test_data['label'].append(df.iloc[i]['label'])\n",
    "            countdown = 0\n",
    "        elif countdown == 0 and checkNoEvidenceInPara(df.iloc[i-sentsInPara:i+1]['label']):\n",
    "            test_data['hadm'].append((int(df.iloc[i]['hadmid']), df.iloc[i]['start_pos']))\n",
    "            test_data['text'].append(' '.join(df.iloc[i-sentsInPara:i+1]['text']))\n",
    "            test_data['label'].append(0)\n",
    "            countdown = sentsInPara\n",
    "        elif countdown > 0:\n",
    "            countdown -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9937b7-8b12-430f-9eb4-b0003a43a663",
   "metadata": {},
   "outputs": [],
   "source": [
    "## EVIDENCE\n",
    "train_df = pd.DataFrame.from_dict(train_data)\n",
    "val_df = pd.DataFrame.from_dict(val_data)\n",
    "test_df = pd.DataFrame.from_dict(test_data)\n",
    "\n",
    "train_df['label'] = train_df.apply(lambda x: 1 if x['label'] != 0 else 0, axis=1)\n",
    "val_df['label'] = val_df.apply(lambda x: 1 if x['label'] != 0 else 0, axis=1)\n",
    "test_df['label'] = test_df.apply(lambda x: 1 if x['label'] != 0 else 0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbb7fd6-370d-440d-8ec3-9c2270bd4990",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SA TYPES\n",
    "train_df = pd.DataFrame.from_dict(train_data)\n",
    "val_df = pd.DataFrame.from_dict(val_data)\n",
    "test_df = pd.DataFrame.from_dict(test_data)\n",
    "\n",
    "train_df = train_df[train_df['label'] != 0]\n",
    "val_df = val_df[val_df['label'] != 0]\n",
    "test_df = test_df[test_df['label'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf042ecf-f19c-4a64-9276-de20eccde6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.drop_duplicates(subset='text')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9111e0a9-9d38-429b-8a2d-4158018c53f9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Train fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce209f5b-dfc8-4462-9267-995843e7a9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([train_df, val_df, test_df])\n",
    "txt = '\\n'.join([l for l in df['text'] if l != '.'])\n",
    "\n",
    "with open(os.path.join(DATA_DIR, \"fasttext\", 'fasttext-train-text.txt'), \"w\") as f:\n",
    "    f.write(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662d20d0-9800-4a36-b9c8-3119f2339541",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model = fasttext.train_unsupervised(os.path.join(DATA_DIR, \"fasttext\", 'fasttext-train-text.txt'), minn=2, dim=100, epoch=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fec490-ec6f-4d87-b0bd-ec49f00e0040",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee45c32d-c0c9-4284-91b4-0dc816322ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = nltk.stem.snowball.SnowballStemmer(language='english')\n",
    "ft = fasttext.load_model('tools/fasttext_stemmed.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dee7e57-e006-424b-9167-3d27bc9a7acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1, w2 = stemmer.stem('sucidal'), stemmer.stem('suicidial')\n",
    "vector1, vector2 = ft.get_word_vector(w1), ft.get_word_vector(w2)\n",
    "np.dot(vector1, vector2) / (np.linalg.norm(vector1) * np.linalg.norm(vector2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491ef5bd-4dae-4af9-ba02-c998fe343bda",
   "metadata": {
    "tags": []
   },
   "source": [
    "# LSTM + Fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5db7528-38bb-4377-b17c-d7d91ba92e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c917d1db-33f3-4da2-8c4f-4a495a8f813f",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {'seed': 42, 'batch_size': 256, 'epochs': 50, 'log_every': 64, 'lr': 0.005, 'dropout': 0.3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad99424-1fd8-4f96-b999-686c3a896459",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import copy\n",
    "import os\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything(args['seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90327b92-ed16-4238-9fd5-5510bbe968ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, dims: list, layers=2, act=nn.LeakyReLU(), dropout_p=0.3, keep_last_layer=False):\n",
    "        super(MLP, self).__init__()\n",
    "        assert len(dims) == layers + 1 \n",
    "        self.layers = layers\n",
    "        self.act = act\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.keep_last = keep_last_layer\n",
    "\n",
    "        self.mlp_layers = nn.ModuleList([])\n",
    "        for i in range(self.layers):\n",
    "            self.mlp_layers.append(nn.Linear(dims[i], dims[i + 1]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.mlp_layers) - 1):\n",
    "            x = self.dropout(self.act(self.mlp_layers[i](x)))\n",
    "        if self.keep_last:\n",
    "            x = self.mlp_layers[-1](x)\n",
    "        else:\n",
    "            x = self.act(self.mlp_layers[-1](x))\n",
    "        return x\n",
    "\n",
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, input_dim=300, hidden_dim=64, output_dim=0, layers=1, act=nn.LeakyReLU(), dropout=0.3):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bi_lstm = nn.LSTM(input_dim, hidden_dim, proj_size=output_dim, num_layers=layers, batch_first=True, dropout=dropout, bidirectional=True)\n",
    "        self.mlp = MLP([hidden_dim*2, hidden_dim, 4], layers=2, act=act, dropout_p=dropout, keep_last_layer=True)\n",
    "    \n",
    "    def forward(self, texts):\n",
    "        docs = []\n",
    "        for t in texts:\n",
    "            words = [stemmer.stem(w) for w in word_tokenize(t)]\n",
    "            vector = torch.stack([torch.from_numpy(ft.get_word_vector(w)) for w in words])\n",
    "            docs.append(vector)\n",
    "\n",
    "        x = nn.utils.rnn.pack_sequence(docs, enforce_sorted=False)\n",
    "        _, (hn, cn) = self.bi_lstm(x)\n",
    "        x = hn.permute(1, 2, 0).reshape(len(texts), self.hidden_dim*2)\n",
    "        # x_copy = x.clone()\n",
    "        x = self.mlp(x)\n",
    "        return x#, x_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e350a9-7033-410a-a232-8b226658a42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft = fasttext.load_model('tools/fasttext_stemmed_300.bin')\n",
    "stemmer = nltk.stem.snowball.SnowballStemmer(language='english')\n",
    "\n",
    "def train_e2e(train_dataloader, model, optimizer, loss_func):\n",
    "    model.train()\n",
    "    loss, batch_num = 0, 0\n",
    "    print_epo = args['log_every']\n",
    "    refs, preds = [], []\n",
    "\n",
    "    for i in range(0, len(train_dataloader), args['batch_size']):\n",
    "        data = train_dataloader[i:i+args['batch_size']]\n",
    "        batch_loss, scores, labels = train_e2e_batch(data, model, optimizer, loss_func)\n",
    "        loss += batch_loss\n",
    "        batch_num += 1\n",
    "\n",
    "        refs.extend(labels)\n",
    "        preds.extend([np.argmax(score.detach().numpy()) for score in scores])\n",
    "\n",
    "        if i % print_epo == 0:\n",
    "            print(\"Batch {}, Loss: {}\".format(batch_num, loss / batch_num))\n",
    "            \n",
    "    return loss / batch_num, f1_score(refs, preds, average='macro'), precision_score(refs, preds, average='macro'), recall_score(refs, preds, average='macro')\n",
    "\n",
    "def train_e2e_batch(data, model, optimizer, loss_func):\n",
    "    optimizer.zero_grad()\n",
    "    texts = list(data['text'])\n",
    "    labels = torch.Tensor(list(data['label_num'])).to(dtype=torch.int64)\n",
    "        \n",
    "    x = model(texts)\n",
    "    loss = loss_func(x, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.data, torch.softmax(x, dim=-1), labels\n",
    "\n",
    "def val_e2e(val_dataloader, model, loss_func, mode='val'):\n",
    "    model.eval()\n",
    "    loss, batch_num = 0, 0\n",
    "    refs, preds = [], []\n",
    "\n",
    "    for i in range(0, len(val_dataloader), args['batch_size']):\n",
    "        data = val_dataloader[i:i+args['batch_size']]\n",
    "        batch_loss, scores, labels = val_e2e_batch(data, model, optimizer, loss_func, mode)\n",
    "        loss += batch_loss\n",
    "        batch_num += 1\n",
    "\n",
    "        refs.extend(labels)\n",
    "        preds.extend([np.argmax(score.detach().numpy()) for score in scores])\n",
    "        \n",
    "    if mode != 'val':\n",
    "        return loss / batch_num, refs, preds\n",
    "    return loss / batch_num, f1_score(refs, preds, average='macro'), precision_score(refs, preds, average='macro'), recall_score(refs, preds, average='macro')\n",
    "    \n",
    "text2vec = {}\n",
    "def val_e2e_batch(data, model, optimizer, loss_func, mode):\n",
    "    texts = list(data['text'])\n",
    "    labels = torch.Tensor(list(data['label_num'])).to(dtype=torch.int64)\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        x = model(texts)\n",
    "        loss = loss_func(x, labels)\n",
    "    \n",
    "    for tex, vec in zip(texts, x_copy):\n",
    "        if tex not in text2vec:\n",
    "            text2vec[tex] = vec.numpy()\n",
    "    return loss.data, torch.softmax(x, dim=-1), labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d6b074-dbed-44ca-9f89-ecdf5113ce96",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = {}\n",
    "def category_labels(label):\n",
    "    global categories\n",
    "    if label == 0: label = 'SI'\n",
    "    if label in categories:\n",
    "        return categories[label]\n",
    "    categories[label] = len(categories)\n",
    "    return categories[label]\n",
    "\n",
    "labels = {}\n",
    "labels['_train'] = train_df['label'].apply(category_labels)\n",
    "labels['_val'] = val_df['label'].apply(category_labels)\n",
    "labels['_test'] = test_df['label'].apply(category_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47462cea-2186-410a-8141-5103151deac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_class_weights(y):\n",
    "    unique_classes, class_counts = np.unique(y, return_counts=True)\n",
    "    total_samples = len(y)\n",
    "    class_weights = {}\n",
    "\n",
    "    for class_label, class_count in zip(unique_classes, class_counts):\n",
    "        class_weight = total_samples / (2.0 * class_count)\n",
    "        class_weights[class_label] = class_weight\n",
    "\n",
    "    return class_weights\n",
    "\n",
    "class_weights = calculate_class_weights(labels['_train'])\n",
    "# class_weights = calculate_class_weights(train_df['label'])\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22307415-ae34-4ac2-9370-568b916f0187",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels['train'] = train_df['label_num'] = labels['_train']\n",
    "labels['val'] = val_df['label_num'] = labels['_val']\n",
    "labels['test'] = test_df['label_num'] = labels['_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03b7731-593f-4ae6-96af-b73301b1a579",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BiLSTM()\n",
    "optimizer = torch.optim.Adam([{'params': model.parameters()}], lr=args['lr'], weight_decay=1e-5)\n",
    "loss_func = nn.CrossEntropyLoss(weight=torch.tensor(list(class_weights.values()),dtype=torch.float))\n",
    "params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b321d0f-e0ad-4375-9684-ffff845395c9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_r2, best_loss = 0, 10000\n",
    "history = {'val_loss': [], 'loss': []}\n",
    "model_state_dicts = {'s': []}\n",
    "trainset, valset = train_df.copy(), val_df.copy()\n",
    "\n",
    "for i in range(args['epochs']):\n",
    "    print(\"Epoch {}\".format(i))\n",
    "    trainset.sample(frac=1)\n",
    "    \n",
    "    for stt in range(0, len(trainset), 10000):\n",
    "        trainss = trainset.iloc[stt:stt+10000]\n",
    "        loss, F, P, R = train_e2e(trainss, model, optimizer, loss_func)\n",
    "        history['loss'].append(loss.cpu())\n",
    "        print(\"At Epoch {}, Train Loss: {}, F: {}, P: {}, R: {}\".format(i, loss, F, P, R))\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        loss, F, P, R = val_e2e(valset, model, loss_func)\n",
    "        torch.cuda.empty_cache()\n",
    "        history['val_loss'].append(loss.cpu())\n",
    "        print(\"At Epoch {}, Val Loss: {}, F: {}, P: {}, R: {}\".format(len(model_state_dicts['s']), loss, F, P, R))\n",
    "\n",
    "        model_state_dicts['s'].append(copy.deepcopy(model.state_dict()))\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            print(\"Epoch {} stt {} Has best R2 Score of {}\".format(i, len(model_state_dicts['s'])-1, best_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ae36d2-cf06-4bab-b79c-53a9071da186",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history['loss'][:])\n",
    "plt.plot(history['val_loss'][:])\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training', 'Valid'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f277b133-c893-4538-b565-f912e420b4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('checkpoints/SA_lstm_noevidence_sent.mdl'), strict=False)\n",
    "loss, refs, preds = val_e2e(test_df, model, loss_func, mode='test')\n",
    "print(classification_report(refs, preds))\n",
    "\n",
    "def groupLabels(labels):\n",
    "    labels = labels.copy()\n",
    "    for i, l in enumerate(labels):\n",
    "        if l == 3:\n",
    "            labels[i] = 2\n",
    "    return labels\n",
    "print(classification_report(groupLabels(refs), groupLabels(preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d257c3ca-ecaf-48db-a1f2-bc9881656383",
   "metadata": {},
   "outputs": [],
   "source": [
    "def name2Num(x, rev=False):\n",
    "    cates = {'SI': 0, 'SA_positive': 1, 'SA_unsure': 2, 'SA_negative': 3}\n",
    "    for name, num in cates.items():\n",
    "        if rev:\n",
    "            if x == num:\n",
    "                return name\n",
    "        else:\n",
    "            if x == name:\n",
    "                return num\n",
    "    return 0\n",
    "\n",
    "test_df['predicted'] = [name2Num(p, rev=True) for p in preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1401fb7d-137e-4478-b5a4-ad7a737089e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkSAPos(text):\n",
    "    phrases = ['suicide attempt', 'suicide note', 'self inflicted', 'intentional overdose', 'commit suicide']\n",
    "    past_phrases = ['status post', 'previous', 'past', 'prior ', 'history', 'multiple', 'several']\n",
    "    deny_phrases = [' not ', 'denies', 'deny', 'denied', 'never', 'unintentional', 'possible', ' mg ']\n",
    "    for p in phrases:\n",
    "        hasDeny = [(pp in text) for pp in deny_phrases]\n",
    "        if 'suicid' in text and sum(hasDeny[:6]) > 0:\n",
    "            return 'neg'\n",
    "        p1, p2 = p.split()\n",
    "        if p1 in text and p2 in text:\n",
    "            if sum(hasDeny[:6]) > 0:\n",
    "                return 'neg'\n",
    "            if sum(hasDeny) > 0: continue\n",
    "            hasPast = sum([(pp in text) for pp in past_phrases])\n",
    "            if hasPast > 0:\n",
    "                return 'past pos'\n",
    "            return 'present pos'\n",
    "    return False\n",
    "            \n",
    "test_df['predicted'] = [y if y not in ['SA_positive', 'SA_unsure'] else 'SA' for y in test_df['predicted']]\n",
    "for hadm in test_df['hadmid'].unique():\n",
    "    df = test_df[test_df['hadmid'] == hadm]\n",
    "    pos_un = df[df['predicted'] == 'SA']\n",
    "    \n",
    "    posStay = False\n",
    "    for i, row in df[df['predicted'].isin(['SI', 'SA_negative'])].iterrows():\n",
    "        if checkSAPos(row['text']) == 'present pos':\n",
    "            posStay = True\n",
    "            df.loc[i, 'predicted'] = 'SA_positive'\n",
    "        elif checkSAPos(row['text']) == 'past pos':\n",
    "            df.loc[i, 'predicted'] = 'SA_positive'\n",
    "            \n",
    "    for i, row in pos_un.iterrows():\n",
    "        if checkSAPos(row['text']) == 'present pos':\n",
    "            posStay = True\n",
    "            df.loc[i, 'predicted'] = 'SA_positive'\n",
    "        elif checkSAPos(row['text']) == 'past pos':\n",
    "            df.loc[i, 'predicted'] = 'SA_positive'\n",
    "        elif checkSAPos(row['text']) == 'neg':\n",
    "            df.loc[i, 'predicted'] = 'SA_negative'\n",
    "            \n",
    "    if posStay:\n",
    "        df.loc[df['predicted'] == 'SA', 'predicted'] = 'SA_positive'\n",
    "    else:\n",
    "        df.loc[df['predicted'] == 'SA', 'predicted'] = 'SA_unsure'\n",
    "    test_df.loc[test_df['hadmid'] == hadm, 'predicted'] = df['predicted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499c601a-449a-4969-a46b-e8baf828b68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = ['0' if l == 'SI' or l == 0 else l for l in test_df['label']]\n",
    "predicted = ['0' if l == 'SI' or l == 0 else l for l in test_df['predicted']]\n",
    "def groupLabels(labels):\n",
    "    labels = labels.copy()\n",
    "    for i, l in enumerate(labels):\n",
    "        if l == 'SA_unsure':\n",
    "            labels[i] = 'SA_negative'\n",
    "    return labels\n",
    "print(classification_report(label, predicted))\n",
    "print(classification_report(groupLabels(label), groupLabels(predicted)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b9a6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('checkpoints/SA_lstm_fasttext.mdl'), strict=False)\n",
    "loss, refs, preds = val_e2e(train_df, model, loss_func, mode='test')\n",
    "print(classification_report(refs, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d778f49-d5bf-425b-8ef6-f4a7bf16ff2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('checkpoints/SA_lstm_fasttext.pkl', 'wb') as fout:\n",
    "    pickle.dump(text2vec, fout)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
